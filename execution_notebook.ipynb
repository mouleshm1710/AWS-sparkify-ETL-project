{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) CREATE REDSHIFT CLUSTER (RESOURCE): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to the AWS services is successfull \n",
      "\n",
      "Lets Delete Existing IAM role: \n",
      "Deletion of Role is successful\n",
      "\n",
      "Lets Create a new IAM role: \n",
      "Creation of Role is successful\n",
      "\n",
      "Lets attach role Policy: \n",
      "Policy Attached Successfully\n",
      "\n",
      "Lets Delete any existing cluster: \n",
      "cluster deleted initiated...\n",
      "\n",
      "Current Cluster Status: deleting\n",
      "Retrying in 15 Seconds\n",
      "Current Cluster Status: deleting\n",
      "Retrying in 15 Seconds\n",
      "Current Cluster Status: deleting\n",
      "Retrying in 15 Seconds\n",
      "Current Cluster Status: deleting\n",
      "Retrying in 15 Seconds\n",
      "Current Cluster Status: deleting\n",
      "Retrying in 15 Seconds\n",
      "Current Cluster Status: deleting\n",
      "Retrying in 15 Seconds\n",
      "Cluster has been deleted successfully!\n",
      "\n",
      "Lets Create the cluster: \n",
      "Cluster Created Successfully !!\n",
      "\n",
      "Checking the cluster status: \n",
      "\n",
      "Current Cluster Status: creating\n",
      "Current Cluster Status: creating\n",
      "Current Cluster Status: creating\n",
      "Current Cluster Status: creating\n",
      "Current Cluster Status: creating\n",
      "Current Cluster Status: available\n",
      "Cluster is available!\n",
      "\n",
      "ec2.SecurityGroup(id='sg-028ecd31d2819183d')\n",
      "Rule already exists: ALLOW tcp from 0.0.0.0/0 on port 5439.\n",
      "\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "!python create_cluster_resources.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CREATE TABLES (STAGING & ANALYTICAL) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop table IF EXISTS stagingevents\n",
      "Table 'stagingevents' dropped successfully\n",
      "\n",
      "drop table IF EXISTS stagingsongs\n",
      "Table 'stagingsongs' dropped successfully\n",
      "\n",
      "drop table IF EXISTS songplays\n",
      "Table 'songplays' dropped successfully\n",
      "\n",
      "drop table IF EXISTS users\n",
      "Table 'users' dropped successfully\n",
      "\n",
      "drop table IF EXISTS songs\n",
      "Table 'songs' dropped successfully\n",
      "\n",
      "drop table IF EXISTS artists\n",
      "Table 'artists' dropped successfully\n",
      "\n",
      "drop table IF EXISTS time\n",
      "Table 'time' dropped successfully\n",
      "\n",
      " CREATE TABLE stagingevents (artist VARCHAR(255),auth VARCHAR(255),firstName VARCHAR(255),gender CHAR(1),iteminsession INT,lastName VARCHAR(255),length FLOAT,level VARCHAR(255),location VARCHAR(255),method VARCHAR(255),page VARCHAR(255),registration BIGINT,sessionid BIGINT,song VARCHAR(255),status BIGINT,ts BIGINT,useragent TEXT,userid INT );\n",
      "Table 'stagingevents' created successfully\n",
      "\n",
      "CREATE TABLE stagingsongs (num_songs INT,artist_id VARCHAR(255),artist_latitude FLOAT,artist_longitude FLOAT,artist_location VARCHAR(255),artist_name VARCHAR(255),song_id VARCHAR(255),title VARCHAR(255),duration FLOAT,year INT)\n",
      "Table 'stagingsongs' created successfully\n",
      "\n",
      "CREATE TABLE songplays (songplayid BIGINT IDENTITY(0,1) PRIMARY KEY,starttime TIMESTAMP,userid INT,level VARCHAR(255),songid VARCHAR(255),artistid VARCHAR(255),sessionid BIGINT,location VARCHAR(255),useragent TEXT);\n",
      "Table 'songplays' created successfully\n",
      "\n",
      "CREATE TABLE users (userid INT PRIMARY KEY,firstname VARCHAR(255),lastname VARCHAR(255),gender CHAR(1),level VARCHAR(255));\n",
      "Table 'users' created successfully\n",
      "\n",
      "CREATE TABLE songs (song_id VARCHAR(255) PRIMARY KEY,title VARCHAR(255),artist_id VARCHAR(255),year INT,duration FLOAT);\n",
      "Table 'songs' created successfully\n",
      "\n",
      "CREATE TABLE artists (artist_id VARCHAR(255) PRIMARY KEY,name VARCHAR(255),location VARCHAR(255),latitude FLOAT,longitude FLOAT);\n",
      "Table 'artists' created successfully\n",
      "\n",
      "CREATE TABLE time (starttime TIMESTAMP PRIMARY KEY,hour INT,day INT,week INT,month INT,year INT,weekday INT);\n",
      "Table 'time' created successfully\n",
      "\n",
      "\n",
      "ALL NECESSARY TABLES HAVE BEEN CREATED SUCCESSFULLY !\n"
     ]
    }
   ],
   "source": [
    "!python create_tables.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) FINAL ETL PROCESS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COPY stagingevents FROM 's3://udacity-dend/log_data/' CREDENTIALS 'aws_iam_role=arn:aws:iam::342000412548:role/dwhRole' JSON 's3://udacity-dend/log_json_path.json' COMPUPDATE OFF TRUNCATECOLUMNS EMPTYASNULL BLANKSASNULL REGION 'us-west-2';\n",
      "\n",
      "COPYING DATA FROM S3 TO THE REDSHIFT TABLE 'stagingevents':- \n",
      "Row count before copying data into the stagingevents: 0\n",
      "Row count after copying data into stagingevents: 8056\n",
      "\n",
      "\n",
      "COPY stagingsongs FROM 's3://udacity-dend/song_data/' CREDENTIALS 'aws_iam_role=arn:aws:iam::342000412548:role/dwhRole' JSON 'auto' COMPUPDATE OFF TRUNCATECOLUMNS EMPTYASNULL BLANKSASNULL REGION 'us-west-2';\n",
      "\n",
      "COPYING DATA FROM S3 TO THE REDSHIFT TABLE 'stagingsongs':- \n",
      "Row count before copying data into the stagingsongs: 0\n",
      "Row count after copying data into stagingsongs: 14896\n",
      "\n",
      "\n",
      "INSERT INTO songplays (starttime, userid, level, songid, artistid, sessionid, location, useragent) SELECT     TIMESTAMP 'epoch' + e.ts / 1000 * INTERVAL '1 second' AS starttime,     e.userid AS userid,     e.level AS level,     s.song_id AS songid,     s.artist_id AS artistid,     e.sessionid AS sessionid,     e.location AS location,     e.useragent AS useragent FROM stagingevents e JOIN stagingsongs s ON e.song = s.title AND e.artist = s.artist_name and  e.length = s.duration WHERE e.page = 'NextSong'; \n",
      "INSERTING DATA FROM STAGING TABLES TO THE DIMENSIONAL TABLES 'songplays':- \n",
      "Row count before inserting data into the songplays: 0\n",
      "Row count after inserting data into songplays: 319\n",
      "\n",
      "\n",
      "INSERT INTO users (userid, firstname, lastname, gender, level) SELECT DISTINCT     userid,     firstname,     lastname,     gender,     level FROM stagingevents WHERE userid IS NOT NULL; \n",
      "INSERTING DATA FROM STAGING TABLES TO THE DIMENSIONAL TABLES 'users':- \n",
      "Row count before inserting data into the users: 0\n",
      "Row count after inserting data into users: 105\n",
      "\n",
      "\n",
      "INSERT INTO songs (song_id, title, artist_id, year, duration) SELECT DISTINCT     song_id,     title,     artist_id,    year,     duration FROM stagingsongs where song_id is NOT NULL; \n",
      "INSERTING DATA FROM STAGING TABLES TO THE DIMENSIONAL TABLES 'songs':- \n",
      "Row count before inserting data into the songs: 0\n",
      "Row count after inserting data into songs: 14896\n",
      "\n",
      "\n",
      "INSERT INTO artists (artist_id, name, location, latitude, longitude) SELECT DISTINCT     artist_id,     artist_name AS name,     artist_location AS location,     artist_latitude AS latitude,     artist_longitude AS longitude FROM stagingsongs WHERE artist_id IS NOT NULL; \n",
      "INSERTING DATA FROM STAGING TABLES TO THE DIMENSIONAL TABLES 'artists':- \n",
      "Row count before inserting data into the artists: 0\n",
      "Row count after inserting data into artists: 10025\n",
      "\n",
      "\n",
      "INSERT INTO time (starttime, hour, day, week, month, year, weekday) SELECT DISTINCT     TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second' AS starttime,     EXTRACT(hour FROM (TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second')) AS hour,     EXTRACT(day FROM (TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second')) AS day,     EXTRACT(week FROM (TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second')) AS week,     EXTRACT(month FROM (TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second')) AS month,     EXTRACT(year FROM (TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second')) AS year,     EXTRACT(weekday FROM (TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second')) AS weekday FROM stagingevents WHERE ts IS NOT NULL and page = 'NextSong'; \n",
      "INSERTING DATA FROM STAGING TABLES TO THE DIMENSIONAL TABLES 'time':- \n",
      "Row count before inserting data into the time: 0\n",
      "Row count after inserting data into time: 6813\n",
      "\n",
      "\n",
      "ETL PROCESS COMPLETED SUCCESSFULLY !\n"
     ]
    }
   ],
   "source": [
    "!python etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) DELETE RESOURCES: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete the cluster uncomment below line of code\n",
    "#!python delete_cluster_resources.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
